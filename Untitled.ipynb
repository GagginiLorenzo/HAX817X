{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8534b1f-61c2-43e3-b241-72b47636c601",
   "metadata": {},
   "source": [
    "# A faire du 13/02 au 28/02\n",
    "\n",
    "- Setup l'environement ;\n",
    "- recupéré les données synthétiques ;\n",
    "- coder la methode marginale et tester sur les donée ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527d108-9983-44ee-9ae5-a863e5ab4d3d",
   "metadata": {},
   "source": [
    "### Fait du 13 au 18\n",
    "\n",
    "- Explorer la structure algo du repo, quelques points clef resortis :\n",
    "\n",
    "    * Le coeur des calcules pour la parti simulation est fait en R, soit directement avec des packages R, soit par l'emplois de methode en python utiliser via la librairie reticulate.\n",
    "    \n",
    "    * L'environement python est construit via un yaml, pensez pour ètre fournis a conda/mamba forge\n",
    "    \n",
    "    * L'indication fournis pour l'import des librairies R contien une typo importante : install.packages(installedpackages[count]) est une erreur, le nom de la liste est en fait installed_packages\n",
    "    \n",
    "    * Les données généré pour la simulation ne sont directement pas fournis, il faut reffaire tourner la simmulation pour y accédé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd9f87-598b-40c0-8872-6ee88eed55a5",
   "metadata": {},
   "source": [
    "### Fait du 18 au 24 \n",
    "* Setup un environement propre :\n",
    "     - Ne souhaitant pas installé conda sur ma machine, je run un Docker ubuntu avec conda deja setup: https://hub.docker.com/r/condaforge/mambaforge\n",
    "     - une fois le docker en place, j'y installe les utilitaire necessaire et je clone le repo :\n",
    "        * apt update\n",
    "        * apt upgrade\n",
    "        * apt install fish #plus pratique que bash\n",
    "        * fish\n",
    "        * mamba init fish\n",
    "        * apt install software-properties-common\n",
    "        * add-apt-repository ppa:maveonair/helix-editor\n",
<<<<<<< HEAD
    "        * apt update\n",
    "        * apt install helix # mon editeur de code\n",
    "        * apt install r-base-dev\n",
    "        * git clone https://github.com/achamma723/Group_Variable_Importance && cd\n",
    "        * mamba env create -f requirements_conda.yml\n",
=======
    "        * sudo apt update\n",
    "        * sudo apt install helix # mon editeur de code\n",
    "        * apt install r-base-dev\n",
    "        * git clone {https://github.com/achamma723/Group_Variable_Importance} && cd\n",
    "        * mamba env create -f requirements_conda.ym\n",
>>>>>>> 4bbd90e (Got all data)
    "    - Une première difficulter technique significative se présente : le yaml est structuré de façon un peut spécifique, en imposant que certain paquet soit exclusivement installer via pip.\n",
    "    -  Je ne sais pas s'il sagit d'une erreur de configuration, ou une mauvaise manipulation de ma part, mais cette particularité casse l'installation puisque certain de ces paquets ne sont plus a jour sur pip, il faudrait les installer directement via les chanel conda, mais n'ayant pas trouver de bonne syntax yaml pour ce cas spécifique, j'ai fait un script d'installation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c3516d-d8ab-44a3-86f8-d02b84e46ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sys import stdin\n",
    "from os import system\n",
    "from yaml import load, CLoader\n",
    "\n",
    "\n",
    "def get_dependencies(file_in: str) -> list[str]:\n",
    "    data = load(file_in, CLoader)\n",
    "    dependencies = data['dependencies']\n",
    "    pip_dependencies = [each for each in dependencies if 'pip' in each][1]['pip']\n",
    "    return pip_dependencies\n",
    "\n",
    "def install_package(pakagename: str) -> None:\n",
    "    cmd = f'mamba install {pakagename}'\n",
    "    print(cmd)\n",
    "    system(cmd)\n",
    "\n",
    "def main() -> None:\n",
    "    file_in = stdin.read()\n",
    "    data = get_dependencies(file_in)\n",
    "    for package in data:\n",
    "        install_package(package)\n",
    "\n",
    "file_in = stdin.read()\n",
    "deps = get_dependencies(file_in)\n",
    "for pkg in deps:\n",
    "    install_package(pkg)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f598fb7-8ae3-4659-a44d-8c4898ab63c1",
   "metadata": {},
   "source": [
    "une fois l'environement en place, il faut apporté quelque modification au script principal compute_simmulation :\n",
    "* N_CPU <- ifelse(!DEBUG, 100L, 1L) , changer 100L en fonction du nombre de coeur cpu disponible, garder 100L a fait crash ma machine\n",
    "* On peut choisir la liste des methodes a executé, le reste de la structure s'adapte a la taille de la liste, ne garder que methods <- c(\"marginal\") permet de ne faire tourner que la marginal ( la liste compléte initiale de toute les methodes a pris plus de 6h a tourner sur ma machine )\n",
    "* les données synthétiques son stoquer dans la variale sim_data, pour en avoir le csv rajouter une ligne en fin de scripte :\n",
    "    * write.csv(sim_data,\"data.csv\",row.names=FALSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94a547-1e90-4564-a7a9-591e3c5d6063",
   "metadata": {},
   "source": [
    "### Fait du 25 au 28\n",
    "maintenant qu'on a récupéré les donnée, on peut commencer a coder la methode marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e203b047-49b8-4b78-a208-3be00e263083",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Travaille en cour, pas encore fini !\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "# Read and loads\n",
    "df = pd.read_csv('data.csv')\n",
    "Y=df[\"y\"]\n",
    "X=df.drop(\"y\",axis=1)\n",
    "\n",
    "#random slpit train/pred\n",
    "n = 800 \n",
    "train = X.sample(n, axis=0)\n",
    "\n",
    "indices_in_train = train.index.tolist()\n",
    "pred = X[~X.index.isin(indices_in_train)]\n"
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "markdown",
   "id": "21c8b644-72a0-4798-8d0b-8a6e74ca71ff",
   "metadata": {},
   "source": [
    "# Fait du 28/02 au 03/03\n",
    "\n",
    "Affinage du code pour récupéré les donnée,\n",
    "la première methode n'as pas respecter la structure de donnée du scripts, et les donnnées étaient éronnée :\n",
    "    * Les fonctions de génération contiennent des valeur par défault, ma methode n'aurais pas du fonctionné, les donné que j'ai pu récupéré était alors issu des valeurs du script par défault.\n",
    "    * Ma nouvelle, et plus simple methodes, a était d'injecter une instruction d'écriture juste aprés le chargement des données par le scripts\n",
    "    *Cette aproche permet de ne rien modifier aux paramettres de la methode, et m'assure de l'intégriter des donnée récupérée\n",
    "    *On a désormais accés au 100 tirage de donnée que traite la methodes pour chaqu'un des rho_group tester (0, 0.2, 0.5, 0.8) a rho intra group constant 0.8"
   ]
>>>>>>> 4bbd90e (Got all data)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
